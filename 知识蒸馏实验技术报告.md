# Chronos-2 知识蒸馏实验技术报告

## 1. 实验概述

本报告详细描述了基于 Chronos-2 模型的知识蒸馏实验方法。实验旨在通过知识蒸馏技术，将大型教师模型（Chronos-2）的知识转移到更小的学生模型中，在保持预测性能的同时显著减少模型参数量和计算开销。

### 1.1 实验目标

- **模型压缩**：将大型 Chronos-2 模型压缩为更小的学生模型
- **性能保持**：在模型压缩的同时尽可能保持预测性能
- **时间序列预测**：针对时间序列预测任务进行优化

### 1.2 技术方案

采用**手动蒸馏训练方案**（而非直接使用 TRL GKD），原因如下：
- Chronos-2 使用自定义架构（非标准 Transformer）
- 需要精确控制蒸馏过程以适应时间序列预测任务
- 确保训练过程的稳定性和可控性

## 2. 实验架构

### 2.1 教师模型

- **模型**：Amazon Chronos-2 (`amazon/chronos-2`)
- **架构**：基于 Transformer 的时间序列预测模型
- **特点**：支持多变量时间序列预测，输出分位数预测

### 2.2 学生模型

学生模型通过减小以下维度实现压缩：

```python
student_config_overrides = {
    "num_layers": 3,      # 减少层数（教师模型通常为 6-12 层）
    "d_model": 384,       # 减少隐藏层维度（教师模型通常为 512-768）
    "d_ff": 1536,         # 减少前馈网络维度
    "num_heads": 6,       # 减少注意力头数
}
```

**压缩比**：学生模型参数量约为教师模型的 20-30%

## 3. 数据准备

### 3.1 数据集

#### 训练数据
- **来源**：`datasets/monash_csv_downsmp/` 目录下的所有 CSV 文件
- **处理方式**：
  - 自动扫描目录下所有 `.csv` 文件
  - 自动识别目标列（排除 date/timestamp 等列后的第一个数值列）
  - 处理缺失值和数据长度不足的情况
  - 支持单变量时间序列预测

#### 验证数据
- **来源**：`datasets/ETTh1.csv`
- **用途**：用于验证模型性能，选择最佳模型

#### 测试数据
- **来源**：`datasets/eval/` 目录下的数据集（如 ETTm1、traffic 等）
- **用途**：最终测试和可视化

### 3.2 数据预处理

#### 数据划分
参考 LightGTS 的处理方式：
- **训练集**：80%
- **验证集**：10%
- **测试集**：10%

#### 数据标准化
- 使用 `StandardScaler` 进行标准化
- 使用训练集拟合 scaler，然后应用到验证集和测试集
- 确保数据分布的一致性

#### 滑动窗口采样
- **上下文长度**（context_length）：720
- **预测长度**（horizon）：96
- **步长**（stride）：1（训练时），horizon（验证时）
- 使用滑动窗口创建样本对 `(context, future_target)`

### 3.3 数据集类实现

```python
class Chronos2DistillationDataset(Dataset):
    """
    功能：
    - 从 CSV 文件加载时间序列数据
    - 自动识别目标列
    - 数据划分（train/val/test）
    - 数据标准化
    - 滑动窗口采样
    """
```

## 4. 蒸馏损失函数

### 4.1 损失函数设计

由于时间序列预测是**回归任务**，使用 **MSE（均方误差）损失**而非分类任务中的 KL 散度。

#### 软标签损失（Soft Label Loss）
学生模型的预测应该接近教师模型的预测：

```python
soft_loss = F.mse_loss(student_logits, teacher_logits)
```

#### 硬标签损失（Hard Label Loss）
学生模型的预测应该接近真实标签：

```python
hard_loss = F.mse_loss(student_logits, labels)
```

#### 组合损失
使用加权组合：

```python
total_loss = alpha * soft_loss + (1 - alpha) * hard_loss
```

其中：
- `alpha = 0.5`：蒸馏损失权重
- `alpha = 0.0`：完全使用硬标签（相当于普通训练）
- `alpha = 1.0`：完全使用软标签（完全依赖教师模型）

### 4.2 损失函数特点

- **回归任务适配**：使用 MSE 而非 KL 散度
- **软硬标签平衡**：通过 `alpha` 参数控制软硬标签的权重
- **数值稳定性**：包含 NaN/Inf 检查和损失值限制

## 5. 训练流程

### 5.1 训练器实现

```python
class Chronos2DistillationTrainer:
    """
    功能：
    - 管理教师模型和学生模型
    - 实现蒸馏训练循环
    - 处理数据加载和批处理
    - 计算蒸馏损失
    - 模型保存和评估
    """
```

### 5.2 训练步骤

#### 步骤 1：模型初始化
1. 加载教师模型（Chronos-2）
2. 创建学生模型（基于教师配置，应用压缩参数）
3. 冻结教师模型参数（设置为评估模式，`requires_grad=False`）

#### 步骤 2：数据准备
1. 创建训练数据集和验证数据集
2. 初始化数据加载器（DataLoader）

#### 步骤 3：训练循环
对于每个 epoch：

1. **前向传播**：
   - 教师模型预测（不计算梯度）：`teacher_logits`
   - 学生模型预测：`student_logits`

2. **损失计算**：
   ```python
   # 软标签损失
   distillation_loss = F.mse_loss(student_logits, teacher_logits)
   
   # 硬标签损失
   mse_loss = F.mse_loss(student_logits, future_target)
   
   # 组合损失
   loss = alpha * distillation_loss + (1 - alpha) * mse_loss
   ```

3. **反向传播**：
   - 计算梯度
   - 梯度裁剪（`max_grad_norm = 1.0`）
   - 更新学生模型参数

4. **验证**：
   - 在验证集上评估
   - 保存最佳模型

### 5.3 训练参数

```python
# 训练配置
learning_rate = 1e-5      # 学习率（降低以提高稳定性）
batch_size = 256         # 批次大小
num_epochs = 10          # 训练轮数
alpha = 0.5              # 蒸馏损失权重
max_grad_norm = 1.0      # 梯度裁剪阈值
temperature = 2.0        # 温度参数（当前未使用，保留用于未来扩展）
```

### 5.4 训练稳定性措施

1. **梯度裁剪**：防止梯度爆炸
2. **NaN/Inf 检查**：跳过包含异常值的批次
3. **损失值限制**：跳过损失过大的批次
4. **数值稳定性**：使用 `eps=1e-8` 等参数

## 6. 模型预测与评估

### 6.1 预测流程

1. **加载模型**：从保存路径加载训练好的学生模型
2. **准备数据**：从测试数据集中提取上下文序列
3. **模型预测**：使用 `Chronos2Pipeline.predict_df()` 方法
4. **提取预测值**：从分位数预测中提取中位数（quantile 0.5）

### 6.2 评估指标

使用以下回归指标评估模型性能：

- **MSE（均方误差）**：`MSE = mean((pred - true)^2)`
- **MAE（平均绝对误差）**：`MAE = mean(|pred - true|)`
- **RMSE（均方根误差）**：`RMSE = sqrt(MSE)`

### 6.3 可视化

生成对比可视化图表，包括：
- **上下文数据**：输入的历史序列
- **真实值**：未来的真实值
- **学生模型预测**：学生模型的预测结果
- **教师模型预测**：教师模型的预测结果（用于对比）

支持单变量和多变量预测的可视化。

## 7. 实验配置

### 7.1 完整配置示例

```python
# 教师模型
teacher_model_id = "amazon/chronos-2"
output_dir = "./chronos-2-distilled"

# 数据配置
train_data_dir = "datasets/monash_csv_downsmp"
eval_data_paths = ["datasets/ETTh1.csv"]

# 时间序列参数
context_length = 720
horizon = 96
stride = 1

# 学生模型配置
student_config_overrides = {
    "num_layers": 3,
    "d_model": 384,
    "d_ff": 1536,
    "num_heads": 6,
}

# 训练配置
learning_rate = 1e-5
batch_size = 256
num_epochs = 10
alpha = 0.5
max_grad_norm = 1.0
```

### 7.2 运行方式

#### 训练
```bash
python chronos2_distill_gkd.py
```

#### 测试
```bash
python test_student_model.py
```

## 8. 关键技术点

### 8.1 Chronos-2 架构适配

- **自定义架构**：Chronos-2 使用非标准 Transformer 架构
- **Patch-based 处理**：使用 patch 方式处理时间序列
- **分位数预测**：输出多个分位数的预测结果
- **多变量支持**：支持多变量时间序列预测

### 8.2 蒸馏策略

- **离线蒸馏**：教师模型预测结果作为软标签
- **软硬标签结合**：同时使用教师预测和真实标签
- **回归任务适配**：使用 MSE 损失而非 KL 散度

### 8.3 数据处理

- **自动列识别**：自动识别目标列
- **数据标准化**：使用训练集拟合的 scaler
- **滑动窗口**：高效的样本生成方式
- **异常值处理**：自动跳过包含 NaN/Inf 的样本

## 9. 实验结果

### 9.1 模型压缩效果

- **参数量减少**：学生模型参数量约为教师模型的 20-30%
- **模型大小**：显著减少模型存储空间

### 9.2 性能表现

（根据实际实验结果填写）

- **MSE 比率**：学生模型 MSE / 教师模型 MSE
- **MAE 比率**：学生模型 MAE / 教师模型 MAE
- **RMSE 比率**：学生模型 RMSE / 教师模型 RMSE

### 9.3 可视化结果

生成的可视化图表保存在 `results/` 目录下，包括：
- 单变量预测对比图
- 多变量预测对比图（每个变量单独一张图）

## 10. 实验总结

### 10.1 方法优势

1. **完全兼容**：完全适配 Chronos-2 的自定义架构
2. **精确控制**：可以精确控制蒸馏过程的每个环节
3. **稳定可靠**：包含多种稳定性措施，训练过程稳定
4. **易于扩展**：代码结构清晰，易于修改和扩展

### 10.2 技术亮点

1. **回归任务适配**：针对时间序列预测任务，使用 MSE 损失而非 KL 散度
2. **软硬标签平衡**：通过 `alpha` 参数灵活控制软硬标签权重
3. **数据处理自动化**：自动识别目标列、处理数据划分和标准化
4. **多变量支持**：支持多变量时间序列预测和可视化

### 10.3 未来改进方向

1. **温度参数**：当前未使用温度参数，未来可以探索温度缩放
2. **在线蒸馏**：当前为离线蒸馏，可以探索在线蒸馏方法
3. **多教师蒸馏**：可以探索使用多个教师模型进行蒸馏
4. **渐进式蒸馏**：可以探索渐进式压缩策略

## 11. 参考文献

1. Chronos-2: A Foundation Model for Time Series Forecasting
2. Knowledge Distillation: A Survey
3. LightGTS: A Lightweight Time Series Forecasting Model

## 12. 附录

### 12.1 文件结构

```
MoE/
├── chronos2_distill_gkd.py      # 主蒸馏脚本
├── test_student_model.py         # 测试脚本
├── CHRONOS2_DISTILL_README.md    # 使用说明
├── DISTILLATION_SETUP.md         # 配置说明
└── results/                      # 结果目录
    └── [dataset_name]/          # 各数据集的结果
        └── [variable]_prediction_start[idx].png
```

### 12.2 关键代码片段

#### 蒸馏损失计算
```python
def compute_distillation_loss(
    self,
    student_logits: torch.Tensor,
    teacher_logits: torch.Tensor,
    labels: torch.Tensor
) -> torch.Tensor:
    soft_loss = F.mse_loss(student_logits, teacher_logits)
    hard_loss = F.mse_loss(student_logits, labels)
    total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss
    return total_loss
```

#### 训练循环核心
```python
# 教师模型预测（不计算梯度）
with torch.no_grad():
    teacher_outputs = self.teacher_pipeline.predict(inputs, prediction_length=self.horizon)
    teacher_logits = extract_median_predictions(teacher_outputs)

# 学生模型预测
student_outputs = self.student_model(context=context, group_ids=group_ids, ...)
student_logits = extract_median_predictions(student_outputs)

# 计算损失
distillation_loss = F.mse_loss(student_logits, teacher_logits)
mse_loss = F.mse_loss(student_logits, future_target)
loss = self.alpha * distillation_loss + (1 - self.alpha) * mse_loss

# 反向传播
loss.backward()
torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), self.max_grad_norm)
self.optimizer.step()
```

---

**报告生成时间**：2024年

**实验代码版本**：基于 `chronos2_distill_gkd.py`

**作者**：实验团队

